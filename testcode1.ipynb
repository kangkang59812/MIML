{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for layer in base_model.modules():\n",
    "    print(layer)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = list(base_model.features)[:-1]\n",
    "base_model = nn.Sequential(*base_model)\n",
    "conv1 = nn.Conv2d(512, 512, 1)\n",
    "dropout1 = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 14, 14])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1=base_model(torch.randn(8,3,224,224))\n",
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 14, 14])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2=conv1(a1)\n",
    "a2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 14, 14])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3=dropout1(a2)\n",
    "a3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = nn.Conv2d(512,1024*20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 20480, 14, 14])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a4=conv2(a3)\n",
    "a4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 20, 196])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a5=a4.reshape(-1,1024,20,196)\n",
    "a5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool=nn.MaxPool2d((20,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 1, 196])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a6=maxpool(a5)\n",
    "a6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 196])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a7=a6.reshape(-1,1024,196)\n",
    "a7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 1024])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a8=a7.permute(0,2,1)\n",
    "a8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax=nn.Softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 196, 1024])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a9=softmax(a8)\n",
    "a9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a9[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(a9[0],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 1, 196])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a10=a9.permute(0,2,1).reshape(-1,1024,1,196)\n",
    "a10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool2=nn.MaxPool2d((1,196))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a11=maxpool2(a10)\n",
    "a11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a12=a11.squeeze()\n",
    "a12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=torch.randn(5,5)\n",
    "y=aa**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.1276,  1.3079,  0.0000,  0.0000],\n",
       "        [ 0.0000, 13.4876,  2.0259,  0.0000,  0.0000],\n",
       "        [ 0.7124,  0.0000,  5.8004,  1.1224,  2.9020],\n",
       "        [ 1.4645,  0.0811,  0.0000,  1.8155,  0.1056],\n",
       "        [ 0.0000,  0.0352,  0.0972,  0.2645,  0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax',name='soft'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 625us/step - loss: 2.3834 - acc: 0.1040\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 282us/step - loss: 2.3287 - acc: 0.0940\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 297us/step - loss: 2.3125 - acc: 0.1020\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 296us/step - loss: 2.2997 - acc: 0.1200\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 436us/step - loss: 2.2904 - acc: 0.1100\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 2.2795 - acc: 0.1180\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 2.2717 - acc: 0.1370\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 106us/step - loss: 2.2601 - acc: 0.1380\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 2.2524 - acc: 0.1500\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 2.2414 - acc: 0.1520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10994221, 0.10140774, 0.11080837, ..., 0.09173212, 0.10617281,\n",
       "        0.08699389],\n",
       "       [0.15377744, 0.08422421, 0.09047195, ..., 0.07036699, 0.1217503 ,\n",
       "        0.07718779],\n",
       "       [0.14241292, 0.10160103, 0.09693567, ..., 0.11899488, 0.07380358,\n",
       "        0.07410032],\n",
       "       ...,\n",
       "       [0.09770095, 0.11170392, 0.10249366, ..., 0.1191003 , 0.07754569,\n",
       "        0.10511004],\n",
       "       [0.12710132, 0.08330646, 0.07090042, ..., 0.10710155, 0.07563601,\n",
       "        0.09263028],\n",
       "       [0.09819156, 0.11862726, 0.09668502, ..., 0.12084874, 0.08523441,\n",
       "        0.083705  ]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dummy data\n",
    "import numpy as np\n",
    "import keras\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)\n",
    "\n",
    "dense1_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer('soft').output)\n",
    "#以这个model的预测值作为输出\n",
    "dense1_output = dense1_layer_model.predict(data)\n",
    " \n",
    "dense1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Out[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999994, 0.99999994, 1.        , 0.99999994, 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.99999994, 1.        ,\n",
       "       1.        , 1.        , 0.99999994, 1.        , 0.99999994,\n",
       "       1.0000001 , 0.99999994, 1.        , 0.9999998 , 0.99999994,\n",
       "       1.        , 0.99999994, 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 0.99999994, 0.99999994, 1.        ,\n",
       "       0.9999999 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.9999999 ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.99999994, 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 0.9999999 , 1.0000001 , 1.        , 1.        ,\n",
       "       0.99999994, 0.99999994, 1.0000001 , 1.        , 0.99999994,\n",
       "       1.        , 0.99999994, 0.99999994, 0.99999994, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.9999999 , 1.        , 0.99999994, 1.0000001 , 1.0000001 ,\n",
       "       0.9999999 , 0.9999999 , 1.0000001 , 1.0000001 , 0.9999999 ,\n",
       "       0.99999994, 0.9999999 , 0.99999994, 1.        , 0.9999999 ,\n",
       "       0.99999994, 0.99999994, 0.99999994, 1.        , 1.        ,\n",
       "       0.9999999 , 1.        , 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 0.9999999 , 1.        , 1.0000001 , 0.99999994,\n",
       "       1.0000001 , 0.99999994, 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 0.9999999 , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 0.9999999 , 1.0000001 , 0.99999994, 1.        ,\n",
       "       1.        , 0.99999994, 0.99999994, 1.        , 0.99999994,\n",
       "       1.        , 1.0000001 , 0.99999994, 0.99999994, 1.        ,\n",
       "       0.9999999 , 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.0000001 , 1.        , 0.99999994, 1.        ,\n",
       "       1.        , 0.9999999 , 1.        , 0.99999994, 1.0000001 ,\n",
       "       1.0000001 , 0.99999994, 1.        , 1.0000001 , 0.99999994,\n",
       "       0.99999994, 1.0000001 , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99999994, 1.0000001 , 1.        ,\n",
       "       0.99999994, 1.0000001 , 1.        , 0.9999999 , 0.99999994,\n",
       "       1.        , 0.9999999 , 1.0000001 , 1.        , 1.        ,\n",
       "       0.99999994, 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99999994, 0.9999999 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 0.99999994, 1.        , 1.0000001 , 1.        ,\n",
       "       1.0000001 , 0.99999994, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.0000001 ,\n",
       "       0.9999998 , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       0.99999994, 1.        , 1.0000001 , 0.99999994, 0.99999994,\n",
       "       1.        , 1.        , 0.99999994, 0.99999994, 1.        ,\n",
       "       1.        , 1.0000001 , 1.        , 1.        , 0.9999999 ,\n",
       "       1.        , 1.0000001 , 1.        , 1.0000001 , 1.0000001 ,\n",
       "       1.        , 0.99999994, 1.        , 1.        , 0.99999994,\n",
       "       1.0000001 , 1.0000001 , 0.99999994, 0.99999994, 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.0000001 , 1.        , 0.9999999 , 1.        ,\n",
       "       1.0000001 , 0.9999999 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.        , 1.0000001 , 0.9999999 , 1.        , 1.0000001 ,\n",
       "       1.0000001 , 1.        , 0.99999994, 0.99999994, 1.        ,\n",
       "       0.99999994, 1.0000001 , 0.9999999 , 0.99999994, 1.        ,\n",
       "       1.0000001 , 1.        , 1.        , 1.        , 1.0000001 ,\n",
       "       0.99999994, 0.99999994, 1.0000001 , 0.9999999 , 0.9999999 ,\n",
       "       1.        , 1.0000001 , 1.        , 0.9999999 , 1.0000001 ,\n",
       "       1.        , 1.        , 1.        , 0.9999999 , 1.0000001 ,\n",
       "       1.        , 0.99999994, 1.0000001 , 0.9999999 , 1.0000001 ,\n",
       "       0.99999994, 1.        , 1.        , 1.0000001 , 1.        ,\n",
       "       0.99999994, 0.99999994, 0.99999994, 1.        , 1.0000001 ,\n",
       "       1.0000001 , 0.99999994, 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.0000001 , 1.0000001 , 0.9999999 , 1.0000001 ,\n",
       "       1.0000001 , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.0000001 , 1.        ,\n",
       "       1.        , 1.0000001 , 1.        , 0.9999999 , 1.        ,\n",
       "       0.99999994, 1.        , 0.99999994, 1.0000001 , 1.        ,\n",
       "       1.        , 0.99999994, 0.99999994, 1.        , 1.        ,\n",
       "       0.9999999 , 1.        , 0.9999999 , 1.        , 0.99999994,\n",
       "       1.        , 0.99999994, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 1.0000001 , 1.        , 1.        , 1.0000001 ,\n",
       "       0.99999994, 1.0000001 , 0.99999994, 0.99999994, 0.99999994,\n",
       "       1.        , 1.0000001 , 1.        , 0.99999994, 0.99999994,\n",
       "       0.9999999 , 1.        , 1.0000001 , 1.        , 0.99999994,\n",
       "       0.99999994, 0.99999994, 1.        , 1.0000001 , 0.99999994,\n",
       "       0.99999994, 1.        , 1.0000001 , 0.99999994, 0.99999994,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.0000001 , 1.        ,\n",
       "       0.99999994, 0.99999994, 1.        , 0.99999994, 0.99999994,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.99999994,\n",
       "       0.99999994, 1.        , 1.0000001 , 0.99999994, 1.        ,\n",
       "       1.        , 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 1.0000001 , 1.0000001 , 1.0000001 , 0.99999994,\n",
       "       0.99999994, 1.        , 0.99999994, 0.9999999 , 1.0000001 ,\n",
       "       0.9999999 , 1.        , 1.0000001 , 0.99999994, 1.0000001 ,\n",
       "       1.        , 0.99999994, 1.0000001 , 1.        , 1.0000001 ,\n",
       "       0.99999994, 0.99999994, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.99999994, 1.        , 1.        , 1.        ,\n",
       "       1.0000001 , 1.        , 1.        , 1.        , 0.9999999 ,\n",
       "       1.        , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 0.99999994, 1.        , 1.        , 1.0000001 ,\n",
       "       0.9999999 , 1.        , 0.99999994, 1.0000001 , 0.9999999 ,\n",
       "       0.99999994, 0.99999994, 1.        , 1.        , 1.0000001 ,\n",
       "       0.99999994, 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       0.9999999 , 0.99999994, 1.0000001 , 1.0000001 , 1.        ,\n",
       "       1.        , 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 0.9999998 , 1.0000001 , 1.0000001 , 0.99999994,\n",
       "       1.        , 0.9999999 , 0.99999994, 1.0000001 , 0.99999994,\n",
       "       1.        , 0.99999994, 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.0000001 , 1.        , 0.99999994,\n",
       "       1.        , 0.99999994, 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 1.        , 1.        , 0.9999999 , 0.9999999 ,\n",
       "       1.        , 1.        , 1.        , 1.0000001 , 0.9999999 ,\n",
       "       1.        , 0.9999999 , 1.        , 1.        , 0.9999999 ,\n",
       "       1.        , 1.        , 1.        , 0.99999994, 0.9999999 ,\n",
       "       0.9999999 , 0.9999999 , 1.        , 1.0000001 , 1.        ,\n",
       "       1.0000001 , 1.0000001 , 1.        , 0.99999994, 1.        ,\n",
       "       1.0000001 , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.0000001 , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.9999999 , 1.        ,\n",
       "       0.99999994, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.0000001 , 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.        , 1.0000001 ,\n",
       "       0.99999994, 1.        , 1.0000001 , 0.9999999 , 1.0000001 ,\n",
       "       1.        , 0.99999994, 1.0000001 , 1.        , 1.0000001 ,\n",
       "       1.        , 1.0000001 , 1.0000001 , 0.9999998 , 1.        ,\n",
       "       1.        , 0.9999999 , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 1.0000001 , 0.9999999 , 1.0000001 ,\n",
       "       0.9999999 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.99999994, 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 0.99999994, 0.99999994, 1.        ,\n",
       "       1.        , 0.99999994, 1.        , 1.0000001 , 1.0000001 ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.9999999 , 1.        , 1.        , 0.99999994, 1.        ,\n",
       "       0.9999999 , 0.99999994, 0.99999994, 1.        , 1.        ,\n",
       "       0.9999999 , 1.0000001 , 0.9999999 , 1.        , 0.9999999 ,\n",
       "       1.0000001 , 0.9999999 , 1.        , 1.        , 1.0000001 ,\n",
       "       1.0000001 , 0.9999999 , 0.99999994, 1.0000001 , 1.        ,\n",
       "       1.        , 0.9999999 , 1.0000001 , 1.        , 1.        ,\n",
       "       0.99999994, 0.99999994, 1.        , 0.99999994, 1.        ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 0.99999994, 1.0000001 ,\n",
       "       0.9999999 , 1.        , 0.9999999 , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99999994, 0.9999999 , 1.        ,\n",
       "       1.0000001 , 1.        , 1.0000001 , 0.9999999 , 1.        ,\n",
       "       1.        , 0.99999994, 0.99999994, 1.        , 0.99999994,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.0000001 , 0.99999994, 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.9999999 , 1.        , 1.        , 1.        ,\n",
       "       1.0000001 , 0.99999994, 1.        , 1.        , 1.0000001 ,\n",
       "       1.        , 1.        , 1.        , 0.99999994, 1.0000001 ,\n",
       "       1.        , 1.0000001 , 0.99999994, 1.        , 1.0000001 ,\n",
       "       0.99999994, 1.        , 0.99999994, 1.        , 1.0000001 ,\n",
       "       1.        , 1.0000001 , 0.99999994, 0.9999998 , 1.0000001 ,\n",
       "       1.        , 1.0000002 , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 0.9999999 , 0.99999994, 1.0000001 , 0.99999994,\n",
       "       1.0000001 , 0.99999994, 1.        , 0.9999999 , 1.        ,\n",
       "       0.99999994, 1.0000001 , 1.        , 1.        , 0.9999999 ,\n",
       "       1.        , 0.9999999 , 1.        , 1.        , 0.99999994,\n",
       "       1.0000001 , 0.99999994, 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 0.99999994, 1.        , 0.99999994,\n",
       "       1.        , 1.        , 1.0000001 , 1.0000001 , 0.9999999 ,\n",
       "       1.0000001 , 1.        , 0.9999999 , 1.        , 1.        ,\n",
       "       1.        , 0.9999999 , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.9999999 , 1.        , 0.9999999 ,\n",
       "       1.0000001 , 1.0000001 , 1.        , 0.9999999 , 1.        ,\n",
       "       0.99999994, 1.0000001 , 1.        , 1.        , 0.99999994,\n",
       "       0.99999994, 1.        , 1.        , 1.        , 0.99999994,\n",
       "       0.99999994, 0.9999999 , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.0000001 , 0.9999999 ,\n",
       "       1.        , 1.0000001 , 0.99999994, 0.99999994, 1.        ,\n",
       "       1.        , 0.99999994, 0.99999994, 1.0000001 , 1.        ,\n",
       "       1.        , 1.        , 1.0000001 , 1.        , 0.99999994,\n",
       "       1.0000001 , 0.99999994, 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 0.9999999 , 1.0000001 , 1.        , 1.        ,\n",
       "       0.99999994, 1.        , 1.0000001 , 1.        , 1.0000001 ,\n",
       "       0.99999994, 0.99999994, 0.99999994, 0.9999999 , 1.        ,\n",
       "       1.        , 1.        , 0.99999994, 1.        , 0.99999994,\n",
       "       0.9999999 , 0.99999994, 1.        , 1.        , 1.0000001 ,\n",
       "       0.99999994, 1.        , 1.        , 0.9999999 , 0.99999994,\n",
       "       1.        , 1.        , 1.        , 0.99999994, 1.0000001 ,\n",
       "       1.0000001 , 0.99999994, 1.0000001 , 1.        , 1.0000001 ,\n",
       "       1.        , 1.        , 1.0000001 , 0.99999994, 1.        ,\n",
       "       1.        , 1.0000001 , 1.0000001 , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 0.9999999 , 1.        , 0.99999994,\n",
       "       1.        , 0.99999994, 0.9999999 , 0.99999994, 1.        ,\n",
       "       0.99999994, 0.9999999 , 1.0000001 , 0.99999994, 1.        ,\n",
       "       1.        , 0.99999994, 1.        , 0.99999994, 1.        ,\n",
       "       0.99999994, 1.        , 1.        , 0.9999999 , 1.        ,\n",
       "       0.99999994, 0.99999994, 1.        , 1.0000001 , 1.        ,\n",
       "       0.9999999 , 0.9999998 , 0.99999994, 0.99999994, 0.99999994,\n",
       "       0.99999994, 0.99999994, 0.99999994, 0.99999994, 1.        ,\n",
       "       0.9999999 , 0.99999994, 0.9999999 , 1.        , 0.99999994,\n",
       "       0.9999999 , 1.0000001 , 1.        , 1.        , 1.        ,\n",
       "       1.0000001 , 1.0000001 , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99999994, 0.99999994, 1.        ,\n",
       "       1.        , 0.99999994, 0.99999994, 0.9999999 , 1.        ,\n",
       "       1.        , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000002 ,\n",
       "       0.99999994, 1.        , 1.        , 1.        , 0.9999999 ,\n",
       "       1.0000001 , 1.0000001 , 0.9999999 , 0.99999994, 0.9999998 ,\n",
       "       1.        , 1.        , 1.        , 0.9999999 , 1.0000001 ,\n",
       "       1.        , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.99999994, 1.        ,\n",
       "       0.9999999 , 1.        , 0.99999994, 1.        , 0.9999999 ,\n",
       "       0.99999994, 1.        , 1.0000001 , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.0000001 ,\n",
       "       1.0000001 , 0.99999994, 1.        , 0.9999999 , 1.        ,\n",
       "       1.0000001 , 0.99999994, 1.        , 0.9999999 , 0.9999999 ,\n",
       "       0.99999994, 1.        , 1.        , 0.99999994, 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = torchvision.models.vgg16(pretrained=True)\n",
    "base_model = list(base_model.features)[:-1]\n",
    "base_model = nn.Sequential(*base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 3, 3, 3])\n",
      "1 torch.Size([64])\n",
      "2 torch.Size([64, 64, 3, 3])\n",
      "3 torch.Size([64])\n",
      "4 torch.Size([128, 64, 3, 3])\n",
      "5 torch.Size([128])\n",
      "6 torch.Size([128, 128, 3, 3])\n",
      "7 torch.Size([128])\n",
      "8 torch.Size([256, 128, 3, 3])\n",
      "9 torch.Size([256])\n",
      "10 torch.Size([256, 256, 3, 3])\n",
      "11 torch.Size([256])\n",
      "12 torch.Size([256, 256, 3, 3])\n",
      "13 torch.Size([256])\n",
      "14 torch.Size([512, 256, 3, 3])\n",
      "15 torch.Size([512])\n",
      "16 torch.Size([512, 512, 3, 3])\n",
      "17 torch.Size([512])\n",
      "18 torch.Size([512, 512, 3, 3])\n",
      "19 torch.Size([512])\n",
      "20 torch.Size([512, 512, 3, 3])\n",
      "21 torch.Size([512])\n",
      "22 torch.Size([512, 512, 3, 3])\n",
      "23 torch.Size([512])\n",
      "24 torch.Size([512, 512, 3, 3])\n",
      "25 torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for i,p in enumerate(base_model.named_parameters()):\n",
    "    print(i,p[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReLU' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-43cee2d0c6e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 535\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReLU' object has no attribute 'weight'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "    # If fine-tuning, only fine-tune convolutional blocks 2 through 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model[0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(base_model.children())[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "ReLU(inplace)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "ReLU(inplace)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "ReLU(inplace)\n"
     ]
    }
   ],
   "source": [
    "for c in list(base_model.children())[-6:]:\n",
    "    print(c)\n",
    "    for p in c.parameters():\n",
    "        print(p.shape)\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Out[52][1].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "sub_concept_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(512, 512, 1)),\n",
    "            ('dropout1', nn.Dropout(0.5)), # (-1,512,14,14)\n",
    "            ('conv2', nn.Conv2d(512, 1600, 1)),\n",
    "            ('maxpool1', nn.MaxPool2d((80, 1))),  # input need reshape to (-1,L,K,H*W)\n",
    "            ('softmax1',nn.Softmax(dim = 2)), # reshape input to (-1,L,H*W), # permute(0,2,1)\n",
    "            ('maxpool2',nn.MaxPool2d((1, 196))) # permute(0,2,1) # reshape to (-1,L,1,H*W)\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_concept_layer.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "from collections import OrderedDict\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class MIML(nn.Module):\n",
    "\n",
    "    def __init__(self, L=1024, K=20, batch_size=8, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            L (int):\n",
    "                number of labels\n",
    "            K (int):\n",
    "                number of sub categories\n",
    "        \"\"\"\n",
    "        super(MIML, self).__init__()\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        self.batch_size = batch_size\n",
    "        # pretrained ImageNet VGG\n",
    "        base_model = torchvision.models.vgg16(pretrained=True)\n",
    "        base_model = list(base_model.features)[:-1]\n",
    "        self.base_model = nn.Sequential(*base_model)\n",
    "        self.fine_tune(fine_tune)\n",
    "\n",
    "        self.sub_concept_layer = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(512, 512, 1)),\n",
    "            ('dropout1', nn.Dropout(0.5)),  # (-1,512,14,14)\n",
    "            ('conv2', nn.Conv2d(512, K*L, 1)),\n",
    "            # input need reshape to (-1,L,K,H*W)\n",
    "            ('maxpool1', nn.MaxPool2d((K, 1))),\n",
    "            # reshape input to (-1,L,H*W), # permute(0,2,1)\n",
    "            ('softmax1', nn.Softmax(dim=2)),\n",
    "            # permute(0,2,1) # reshape to (-1,L,1,H*W)\n",
    "            ('maxpool2', nn.MaxPool2d((1, 196)))\n",
    "        ]))\n",
    "        # self.conv1 = nn.Conv2d(512, 512, 1))\n",
    "\n",
    "        # self.dropout1=nn.Dropout(0.5)\n",
    "\n",
    "        # self.conv2=nn.Conv2d(512, K*L, 1)\n",
    "        # # input need reshape to (-1,L,K,H*W)\n",
    "        # self.maxpool1=nn.MaxPool2d((K, 1))\n",
    "        # # reshape input to (-1,L,H*W)\n",
    "        # # permute(0,2,1)\n",
    "        # self.softmax1=nn.Softmax(dim = 2)\n",
    "        # # permute(0,2,1)\n",
    "        # # reshape to (-1,L,1,H*W)\n",
    "        # self.maxpool2=nn.MaxPool2d((1, 196))\n",
    "        # # squeeze()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # IN:(8,3,224,224)-->OUT:(8,512,14,14)\n",
    "        base_out = self.base_model(x)\n",
    "        # C,H,W = 512,14,14\n",
    "        _, C, H, W = base_out.shape\n",
    "        # OUT:(8,512,14,14)\n",
    "\n",
    "        conv1_out = self.sub_concept_layer.dropout1(\n",
    "            self.sub_concept_layer.conv1(base_out))\n",
    "\n",
    "        # shape -> (n_bags, (L * K), n_instances, 1)\n",
    "        conv2_out = self.sub_concept_layer.conv2(conv1_out)\n",
    "        # shape -> (n_bags, L, K, n_instances)\n",
    "        conv2_out = conv2_out.reshape(-1, self.L, self.K, H*W)\n",
    "        # shape -> (n_bags, L, 1, n_instances),remove dim: 1\n",
    "        maxpool1_out = self.sub_concept_layer.maxpool1(conv2_out).squeeze()\n",
    "\n",
    "        # softmax\n",
    "        permute1 = maxpool1_out.permute(0, 2, 1)\n",
    "        softmax1 = self.sub_concept_layer.softmax1(permute1)\n",
    "        permute2 = softmax1.permute(0, 2, 1)\n",
    "        # reshape = permute2.unsqueeze(2)\n",
    "        reshape = permute2.reshape(-1, self.L, 1, H*W)\n",
    "        # shape -> (n_bags, L, 1, 1)\n",
    "        maxpool2_out = self.sub_concept_layer.maxpool2(reshape)\n",
    "        out = maxpool2_out.squeeze()\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        # only fine_tune the last three convs\n",
    "        layer = -6\n",
    "        for p in self.base_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        for c in list(self.base_model.children())[-6:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
